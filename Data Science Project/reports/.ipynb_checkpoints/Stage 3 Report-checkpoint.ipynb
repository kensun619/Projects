{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 838 &mdash; Data Science: Principles, Algorithms, and Applications; Spring 2017 ###\n",
    "\n",
    "#  Stage 3: entity matching #\n",
    "\n",
    "#### Trang Ho, Thomas Ngo, Qinyuan Sun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction ##\n",
    "\n",
    "In this project stage, our team performed matching entities between two tables of education affiliations. The first table was extracted from the Academy of Management Conference (AOM) website, which contains personal information of the conference attendants in the year 2014. The personal information includes (1) individual name, (2) affiliation name, (3) country, (4) states/ province, (5) city, (6) contact numbers, (7) email address. Overall, this table consists of 9,532 entities at the individual level. \n",
    "\n",
    "The second table was extracted from the World Higher Education Database (WHED), which contains information of unique education affiliations worldwide. This table provides information on (1) affiliation name, (2) country, (3) street, (4) city, (5) province/ states, (6) postal code, (7) telephone number, and (8) website address (if available). Overall, this table consists of 17,605 unique entities at the affiliation level.\n",
    "\n",
    "In order to match individuals' affiliations on the first table to affiliations on the second table, we used their overlapped/relevant information: (1) affiliation name, (2) country, (3) province/states, (4) city, (5) website address, (6) individual email address. Our goal here is to get precision score of above 95% and recall score of as high as possible.\n",
    "\n",
    "Subsequently, we carried out the following steps using Magellan:\n",
    "* Pre-processing\n",
    "* Down-sizing the AOM table and the WHED table\n",
    "* Using a blocker to reduce the size of the potential-candidate set\n",
    "* Sampling randomly 500 pairs of potential candidates for labelling\n",
    "* Creating training and testing sets I and J\n",
    "* Training and selecting the best classifier using cross-validation\n",
    "* Evaluating performance on the testing set J\n",
    "\n",
    "More details can be found below.\n",
    "\n",
    "## 2. Matching procedure ##\n",
    "\n",
    "### Step 1. Pre-processing\n",
    "In this step, we cleaned the two datasets by standardizing information on affiliation names, country, state/province, city, email server domain. For example, we standardized states by transforming \"CA\", \"CA - California\", \"California\" to \"california\" on both the AOM table and the WHED data.\n",
    "\n",
    "### Step 2. Down-sizing\n",
    "Initially, we have 9,532 entities on the AOM table and 17,605 entities on the WHED data. After down-sizing, we have 4,000 AOM entities and 4,962 WHED entities\n",
    "\n",
    "### Step 3. Blocking\n",
    "Our blocking consists of the following components:\n",
    "* Blocking all tuple pairs that have different countries\n",
    "* For American affiliations, blocking all tuple pairs that have different province/ states\n",
    "* For all affiliations, blocking all tuple pairs that have neither (1) any overlap between AOM email domain and WHED affiliation website domain nor (2) sufficient overlap coefficient (i.e. greater than 0.5) between affiliation names\n",
    "\n",
    "As a result, we reduced the size of our candidate set from 19,848,000 (=4,000 x 4,962) to 126,516. \n",
    "\n",
    "This step took approximately 9 minutes.\n",
    "\n",
    "### Step 4. Sampling for labelling\n",
    "We initially sampled randomly 500 tuple pairs from the set of 126,516 potential candidates. After labeling, we dropped 22 cases due to ambiguity of the AOM information. Consequently, we had 478 tuple pairs with a density of approximately 34%.\n",
    "\n",
    "We spent about 5 hours on labelling data.\n",
    "\n",
    "### Step 5. Creating training & testing sets\n",
    "We split the sample set into training and testing sets. As a result, each set has 239 tuple pairs.\n",
    "\n",
    "|                | Num. of entities | Num. of positive examples  | Num. of negative examples|\n",
    "| -------------  |:----------------:| :-------------:            | :-------------:          |\n",
    "| Training Set I | 239              |     81 (33.8%)             |  158 (66.2%)             |\n",
    "| Testing Set J  | 239              |     73 (30.5%)             |  166 (69.5%)             |\n",
    "| Total          | 478              |     154                    |  324                     |\n",
    "\n",
    "\n",
    "### Step 6. Training and selecting the best classifiers\n",
    "We used 6 learning methods for training on set I using 5-fold cross validation. The methods include: (1) Decision Tree, (2) Random Forest, (3) SVM, (4) Naive Bayes, (5) Logistic Regression, and (6) Linear Regression. Our classifiers use 34 features, and below is the first-attempt accuracy performance of our classifiers on the training set I:\n",
    "\n",
    "| Machine Learning Algorithm| Avg. CV Precision| Avg. CV Recall |     F1    |\n",
    "| ------------------------- |:----------------:| :-------------:|:---------:|\n",
    "| Decsion Tree              | 0.88             |     0.87       | 0.87      | \n",
    "| Random Forest             | 0.95             |     0.90       | 0.92      | \n",
    "| Support Vector Machine    | 0.96             |     0.51       | 0.63      |\n",
    "| Naive Bayes               | 0.96             |     0.51       | 0.63      | \n",
    "| Logistic Regression       | 0.89             |     0.89       | 0.88      | \n",
    "| Linear Regression         | 0.93             |     0.84       | 0.88      | \n",
    "\n",
    "\n",
    "For its simplicity and accuracy performance, we selected the Random Forest learning method for our testing.\n",
    "\n",
    "This step took about 1 minute.\n",
    "\n",
    "### Step 7. Evaluating performance\n",
    "\n",
    "We trained the classifier with all the training examples and tested on the testing examples. The results are shown in the following table. \n",
    "\n",
    "|Type  |Precision| Recall |F1  |\n",
    "| ---- |:-------:|:------:|:--:|\n",
    "|TRAIN |1.00     | 0.98   |0.99|\n",
    "|TEST  |0.99     | 0.91   |0.95|\n",
    "\n",
    "## 3. Links ##\n",
    "\n",
    "[Link](https://github.com/TrangHo/cs838-code/tree/master/src) to source code directory\n",
    "\n",
    "[Link](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files) to data directory:\n",
    "\n",
    "* a- Source table: [_aom.csv](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/_aom.csv), [_whed.csv](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/_whed.csv)\n",
    "\n",
    "* b- Preprocessed table: [_aom_cleaned.csv](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/_aom_cleaned.csv), [_whed_cleaned.csv](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/_whed_cleaned.csv)\n",
    "\n",
    "* c- Pairs that survive the blocking step: [matching_pairs_table_overlap3_emailserver.pkl](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/matching_pairs_table_overlap3_emailserver.pkl)\n",
    "\n",
    "* d- Golden data: [golden_data_labeled_nomissing.csv](https://github.com/TrangHo/cs838-spring2017/tree/master/stage3/csv_files/golden_data_labeled_nomissing.csv)\n",
    "\n",
    "## 4. Comments on Magellan ##\n",
    "\n",
    "Overall Magellan provides a lot of helpful tools to do entity matching. However, we have a lot of trouble setting up PyQt4 since it seems to be deprecated: official page of Qt only provides Qt5 and PyPi only provides install formula for PyQt5. Hence, we have forked the git repo for [py_entitymatching](https://github.com/anhaidgroup/py_entitymatching) to update the code to work with PyQt5 instead of PyQt4. You can find the forked repo [here](https://github.com/TrangHo/py_entitymatching) and the changes are in branch [support_pyqt5](https://github.com/TrangHo/py_entitymatching/tree/support_pyqt5). We will update the documents and make a pull request in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
